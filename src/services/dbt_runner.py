# stdlib
import json
import logging
import subprocess
from dataclasses import dataclass
from typing import ClassVar, Dict, List, Optional, Set

# first party
from src.config import Config
from src.interfaces.dbt import DbtRunnerProtocol
from src.services.discovery_client import DiscoveryClient

logger = logging.getLogger(__name__)


@dataclass
class DbtRunner(DbtRunnerProtocol):
    """
    Service for executing dbt commands and processing their results.

    This class implements the DbtRunnerProtocol interface, handling all
    interactions with the dbt CLI, including compiling models and retrieving
    compilation results.

    Attributes:
        config: Configuration object containing dbt Cloud settings
        _discovery_client: Optional client for making Discovery API requests
        DBT_COMMANDS: Class-level constant defining available dbt commands
    """

    config: Config
    _discovery_client: Optional[DiscoveryClient] = None

    DBT_COMMANDS: ClassVar[Dict[str, List[str]]] = {
        "compile": [
            "dbt",
            "compile",
            "-s",
            "state:modified,resource_type:model",
            "--favor-state",
        ],
        "ls": [
            "dbt",
            "ls",
            "--resource-type",
            "model",
            "--select",
            "state:modified+",
            "--output",
            "json",
        ],
    }

    def __post_init__(self) -> None:
        """Initialize the Discovery API client if not provided."""
        if self._discovery_client is None:
            self._discovery_client = DiscoveryClient(self.config)

    def compile_models(self) -> None:
        """
        Compile modified models using dbt.

        Executes the dbt compile command for modified models and raises
        a RuntimeError if compilation fails.

        Raises:
            RuntimeError: If the dbt compile command fails
        """
        logger.debug(
            "Compiling modified models",
            extra={"command": " ".join(self.DBT_COMMANDS["compile"])},
        )

        result = subprocess.run(
            self.DBT_COMMANDS["compile"],
            capture_output=True,
            text=True,
        )

        if result.returncode != 0:
            logger.error(
                "Failed to compile models",
                extra={"return_code": result.returncode, "stderr": result.stderr},
            )
            raise RuntimeError("Failed to compile models")

        logger.info("Successfully compiled modified models")

    def get_target_compiled_code(self) -> Dict[str, Dict[str, str]]:
        """
        Get compiled code from run_results.json.

        Reads and parses the run_results.json file generated by dbt compile
        to extract the compiled SQL code for modified models.

        Returns:
            Dict[str, Dict[str, str]]: Dictionary mapping node IDs to their
                                      properties, including compiled code
        """
        logger.debug("Reading run_results.json for compiled code")

        try:
            with open("target/run_results.json") as rr:
                run_results_json = json.load(rr)

            modified_nodes = {}
            for result in run_results_json.get("results", []):
                relation_name = result.get("relation_name")
                if relation_name is not None:
                    unique_id = result["unique_id"]
                    modified_nodes[unique_id] = {
                        "unique_id": unique_id,
                        "target_code": result["compiled_code"],
                    }

            logger.info(
                "Retrieved target compiled code",
                extra={"node_count": len(modified_nodes)},
            )
            return modified_nodes

        except FileNotFoundError:
            logger.error("run_results.json not found")
            return {}
        except json.JSONDecodeError as e:
            logger.error("Failed to parse run_results.json", extra={"error": str(e)})
            return {}

    def get_source_compiled_code(
        self, unique_ids: List[str]
    ) -> Dict[str, Dict[str, str]]:
        """
        Get compiled code from the deferring environment.

        Retrieves the compiled SQL code for specified nodes from the
        dbt Cloud environment using the Discovery API.

        Args:
            unique_ids: List of node unique IDs to fetch compiled code for

        Returns:
            Dict[str, Dict[str, str]]: Dictionary mapping node IDs to their
                                      properties, including compiled code
        """
        logger.debug(
            "Getting source compiled code", extra={"unique_id_count": len(unique_ids)}
        )

        if not unique_ids:
            logger.debug("No unique IDs provided")
            return {}

        return self._discovery_client.get_compiled_code(
            self.config.dbt_cloud_environment_id, unique_ids
        )

    def get_all_unique_ids(self, modified_unique_ids: List[str]) -> Set[str]:
        """
        Get all unique IDs affected by the given models, excluding the input models themselves.
        Only returns downstream dependent nodes.
        """
        logger.debug(
            "Finding affected nodes",
            extra={
                "command": " ".join(self.DBT_COMMANDS["ls"]),
                "modified_count": len(modified_unique_ids),
            },
        )

        result = subprocess.run(
            self.DBT_COMMANDS["ls"],
            capture_output=True,
            text=True,
        )

        if result.returncode != 0:
            logger.error(
                "Failed to list models",
                extra={"return_code": result.returncode, "stderr": result.stderr},
            )
            return set()

        unique_ids = set()
        for line in result.stdout.split("\n"):
            if not line:
                continue
            json_str = line[line.find("{") : line.rfind("}") + 1]
            try:
                data = json.loads(json_str)
                if "unique_id" in data:
                    unique_id = data["unique_id"]
                    if unique_id not in modified_unique_ids:
                        unique_ids.add(unique_id)
            except json.JSONDecodeError:
                continue

        logger.info(
            "Found affected nodes",
            extra={
                "affected_count": len(unique_ids),
                "modified_count": len(modified_unique_ids),
            },
        )
        return unique_ids
